{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5e3290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Imed\\anaconda3\\envs\\plonk\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FUNCTION SIGNATURE ---\n",
      "(images, batch_size=None, x_N=None, num_steps=None, scheduler=None, cfg=0, generator=None)\n",
      "\n",
      "--- DOCUMENTATION ---\n",
      "Sample from the model given conditioning.\n",
      "\n",
      "        Args:\n",
      "            images: Conditioning input (image or list of images)\n",
      "            batch_size: Number of samples to generate (inferred from cond if not provided)\n",
      "            x_N: Initial noise tensor (generated if not provided)\n",
      "            num_steps: Number of sampling steps (uses default if not provided)\n",
      "            sampler: Custom sampler function (uses default if not provided)\n",
      "            scheduler: Custom scheduler function (uses default if not provided)\n",
      "            cfg: Classifier-free guidance scale (default 15)\n",
      "            generator: Random number generator\n",
      "\n",
      "        Returns:\n",
      "            Sampled GPS coordinates after postprocessing\n",
      "        \n",
      "\n",
      "--- SAMPLER ATTRIBUTES ---\n",
      "['__annotations__', '__builtins__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from plonk import PlonkPipeline\n",
    "\n",
    "# 1. Load (if not already loaded)\n",
    "if 'pipeline' not in locals():\n",
    "    pipeline = PlonkPipeline(\"nicolas-dufour/PLONK_OSV_5M\")\n",
    "\n",
    "# 2. Reveal the EXACT signature of the call function\n",
    "print(\"--- FUNCTION SIGNATURE ---\")\n",
    "print(inspect.signature(pipeline.__call__))\n",
    "\n",
    "# 3. Reveal the documentation (if the author wrote it)\n",
    "print(\"\\n--- DOCUMENTATION ---\")\n",
    "print(pipeline.__call__.__doc__)\n",
    "\n",
    "# 4. Check the 'sampler' object \n",
    "# It might control the number of steps\n",
    "if hasattr(pipeline, \"sampler\"):\n",
    "    print(\"\\n--- SAMPLER ATTRIBUTES ---\")\n",
    "    print(dir(pipeline.sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b141ac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ T√©l√©chargement dans le dossier : ./my_local_clip\n",
      "\n",
      "‚úÖ config.json existe d√©j√† (0.0 MB)\n",
      "‚úÖ vocab.json existe d√©j√† (0.9 MB)\n",
      "‚úÖ merges.txt existe d√©j√† (0.5 MB)\n",
      "‚úÖ special_tokens_map.json existe d√©j√† (0.0 MB)\n",
      "‚úÖ tokenizer_config.json existe d√©j√† (0.0 MB)\n",
      "‚úÖ preprocessor_config.json existe d√©j√† (0.0 MB)\n",
      "‚ö†Ô∏è  model.safetensors existe mais semble incomplet, re-t√©l√©chargement...\n",
      "‚¨áÔ∏è  T√©l√©chargement de model.safetensors...\n",
      "   üì• Reprise depuis 368.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.59G/1.59G [01:20<00:00, 16.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ model.safetensors termin√© ! (1631.3 MB)\n",
      "\n",
      "\n",
      "üéâ T√âL√âCHARGEMENT TERMIN√â !\n",
      "üìÇ Mod√®le disponible dans : c:\\Users\\Imed\\Desktop\\VMI\\Project\\Dev\\plonk\\my_local_clip\n",
      "\n",
      "üìä V√©rification des fichiers :\n",
      "   ‚úì config.json: 0.0 MB\n",
      "   ‚úì vocab.json: 0.9 MB\n",
      "   ‚úì merges.txt: 0.5 MB\n",
      "   ‚úì special_tokens_map.json: 0.0 MB\n",
      "   ‚úì tokenizer_config.json: 0.0 MB\n",
      "   ‚úì preprocessor_config.json: 0.0 MB\n",
      "   ‚úì model.safetensors: 1631.3 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dossier o√π on va stocker le mod√®le\n",
    "local_dir = \"./my_local_clip\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Les fichiers n√©cessaires pour CLIP avec leurs tailles approximatives (pour validation)\n",
    "files_to_download = {\n",
    "    \"config.json\": 4_800,\n",
    "    \"vocab.json\": 1_042_000,\n",
    "    \"merges.txt\": 456_000,\n",
    "    \"special_tokens_map.json\": 400,\n",
    "    \"tokenizer_config.json\": 700,\n",
    "    \"preprocessor_config.json\": 350,\n",
    "    \"model.safetensors\": 1_711_000_000  # ~1.7 GB\n",
    "}\n",
    "\n",
    "base_url = \"https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/\"\n",
    "\n",
    "print(f\"üìÅ T√©l√©chargement dans le dossier : {local_dir}\\n\")\n",
    "\n",
    "for filename, expected_size in files_to_download.items():\n",
    "    url = base_url + filename\n",
    "    dest_path = os.path.join(local_dir, filename)\n",
    "    \n",
    "    # V√©rifier si le fichier existe et a une taille raisonnable\n",
    "    if os.path.exists(dest_path):\n",
    "        actual_size = os.path.getsize(dest_path)\n",
    "        # Tol√©rance de 10% sur la taille\n",
    "        if actual_size > expected_size * 0.9:\n",
    "            print(f\"‚úÖ {filename} existe d√©j√† ({actual_size / (1024*1024):.1f} MB)\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {filename} existe mais semble incomplet, re-t√©l√©chargement...\")\n",
    "    \n",
    "    print(f\"‚¨áÔ∏è  T√©l√©chargement de {filename}...\")\n",
    "    \n",
    "    try:\n",
    "        # T√©l√©chargement avec support de reprise\n",
    "        headers = {}\n",
    "        mode = 'wb'\n",
    "        initial_pos = 0\n",
    "        \n",
    "        if os.path.exists(dest_path):\n",
    "            initial_pos = os.path.getsize(dest_path)\n",
    "            headers['Range'] = f'bytes={initial_pos}-'\n",
    "            mode = 'ab'\n",
    "            print(f\"   üì• Reprise depuis {initial_pos / (1024*1024):.1f} MB\")\n",
    "        \n",
    "        response = requests.get(url, stream=True, headers=headers, timeout=30)\n",
    "        \n",
    "        if response.status_code in [200, 206]:  # 200=nouveau, 206=reprise\n",
    "            total_size = int(response.headers.get('content-length', 0)) + initial_pos\n",
    "            \n",
    "            # Utiliser tqdm pour une meilleure barre de progression\n",
    "            with open(dest_path, mode) as file:\n",
    "                with tqdm(\n",
    "                    total=total_size,\n",
    "                    initial=initial_pos,\n",
    "                    unit='B',\n",
    "                    unit_scale=True,\n",
    "                    unit_divisor=1024,\n",
    "                    desc=f\"   {filename}\"\n",
    "                ) as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1 MB chunks\n",
    "                        if chunk:\n",
    "                            file.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "            \n",
    "            print(f\"‚úÖ {filename} termin√© ! ({os.path.getsize(dest_path) / (1024*1024):.1f} MB)\\n\")\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur HTTP {response.status_code} pour {filename}\\n\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Erreur r√©seau pour {filename}: {e}\\n\")\n",
    "        print(f\"   üí° Relancez le script pour reprendre le t√©l√©chargement.\\n\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n‚è∏Ô∏è  T√©l√©chargement interrompu. Relancez le script pour continuer.\\n\")\n",
    "        break\n",
    "\n",
    "print(\"\\nüéâ T√âL√âCHARGEMENT TERMIN√â !\")\n",
    "print(f\"üìÇ Mod√®le disponible dans : {os.path.abspath(local_dir)}\")\n",
    "\n",
    "# V√©rification finale\n",
    "print(\"\\nüìä V√©rification des fichiers :\")\n",
    "for filename in files_to_download.keys():\n",
    "    path = os.path.join(local_dir, filename)\n",
    "    if os.path.exists(path):\n",
    "        size = os.path.getsize(path) / (1024*1024)\n",
    "        print(f\"   ‚úì {filename}: {size:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"   ‚úó {filename}: MANQUANT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a341de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Imed\\anaconda3\\envs\\plonk\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available methods:\n",
      "['compute_likelihood', 'compute_likelihood_grid', 'compute_localizability', 'cond_preprocessing', 'device', 'input_dim', 'manifold', 'model', 'model_path', 'network', 'postprocessing', 'preconditioning', 'sampler', 'scheduler', 'to']\n"
     ]
    }
   ],
   "source": [
    "from plonk import PlonkPipeline\n",
    "import torch\n",
    "\n",
    "# Load pipeline\n",
    "pipeline = PlonkPipeline(\"nicolas-dufour/PLONK_OSV_5M\")\n",
    "\n",
    "# Check available methods\n",
    "print(\"Available methods:\")\n",
    "print([m for m in dir(pipeline) if not m.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad47be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood NFE: 1022\n",
      "‚úÖ compute_likelihood works! Result: tensor([0.6887], device='cuda:0')\n",
      "Computing likelihood over a 19x37 grid (703 points)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Likelihood Grid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:59<00:00, 59.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood NFE: 1334\n",
      "‚ùå Error: 'tuple' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Imed\\Desktop\\VMI\\Project\\Dev\\plonk\\plonk\\models\\samplers\\riemannian_flow_sampler.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood NFE: 938\n",
      "‚úÖ compute_localizability works! Score: 0.6688985824584961\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load a test image (use any image you have)\n",
    "# For now, let's just test with a dummy image\n",
    "test_image = Image.new('RGB', (224, 224), color='blue')\n",
    "\n",
    "# Test 1: Compute likelihood at a specific point (Paris)\n",
    "paris_coords = np.array([[48.8566, 2.3522]])  # [lat, lon]\n",
    "\n",
    "try:\n",
    "    likelihood = pipeline.compute_likelihood(test_image, paris_coords)\n",
    "    print(f\"‚úÖ compute_likelihood works! Result: {likelihood}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test 2: Compute likelihood grid\n",
    "try:\n",
    "    grid = pipeline.compute_likelihood_grid(test_image)\n",
    "    print(f\"‚úÖ compute_likelihood_grid works! Grid shape: {grid.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test 3: Compute localizability\n",
    "try:\n",
    "    localizability = pipeline.compute_localizability(test_image)\n",
    "    print(f\"‚úÖ compute_localizability works! Score: {localizability}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2a8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plonk import PlonkPipeline\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load PLONK pipeline\n",
    "pipeline = PlonkPipeline(\"nicolas-dufour/PLONK_OSV_5M\")\n",
    "pipeline.network.to(\"cuda\")\n",
    "\n",
    "print(\"‚úÖ Pipeline loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b265e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSV-5M dataset structure:\n",
      "datasets/osv5m/osv5m/images\n",
      "datasets/osv5m/osv5m/.gitattributes\n",
      "datasets/osv5m/osv5m/README.md\n",
      "datasets/osv5m/osv5m/osv5m.py\n",
      "datasets/osv5m/osv5m/test.csv\n",
      "datasets/osv5m/osv5m/train.csv\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFileSystem\n",
    "\n",
    "# Browse the dataset structure\n",
    "fs = HfFileSystem()\n",
    "\n",
    "print(\"OSV-5M dataset structure:\")\n",
    "files = fs.ls(\"datasets/osv5m/osv5m\", detail=False)\n",
    "for f in files[:20]:  # First 20 items\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbba0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded test.csv to: C:\\Users\\Imed\\.cache\\huggingface\\hub\\datasets--osv5m--osv5m\\snapshots\\cff33609b56b54d8743b7ee7a416eb8433e9a681\\test.csv\n",
      "\n",
      "üìä Test set size: 210122 images\n",
      "\n",
      "Columns: ['id', 'latitude', 'longitude', 'thumb_original_url', 'country', 'sequence', 'captured_at', 'lon_bin', 'lat_bin', 'cell', 'land_cover', 'road_index', 'drive_side', 'climate', 'soil', 'dist_sea', 'region', 'sub-region', 'city', 'unique_city', 'unique_sub-region', 'unique_region', 'unique_country', 'quadtree_10_1000', 'creator_username', 'creator_id']\n",
      "\n",
      "First few rows:\n",
      "                 id   latitude  longitude  \\\n",
      "0   547473234108938 -16.336027  45.628280   \n",
      "1   826109781317024  50.855687  56.147997   \n",
      "2  1006398440000844  37.956651  14.954485   \n",
      "3  2943891539215481  12.373333  -8.909906   \n",
      "4   122945119799579   7.510295  99.061884   \n",
      "\n",
      "                                  thumb_original_url country  \\\n",
      "0  https://scontent-cdg4-3.xx.fbcdn.net/m1/v/t6/A...      MG   \n",
      "1  https://scontent-cdg4-1.xx.fbcdn.net/m1/v/t6/A...      KZ   \n",
      "2  https://scontent-cdg4-1.xx.fbcdn.net/m1/v/t6/A...      IT   \n",
      "3  https://scontent-cdg4-1.xx.fbcdn.net/m1/v/t6/A...      ML   \n",
      "4  https://scontent-cdg4-3.xx.fbcdn.net/m1/v/t6/A...      TH   \n",
      "\n",
      "                 sequence    captured_at  lon_bin  lat_bin      cell  ...  \\\n",
      "0  Z2sJMyDVAtHBu7UFGvnYXg  1660909818797       61       28  (61, 28)  ...   \n",
      "1  6cTpNBSYQH6SeDsfz_0mEQ  1548919530404       64       78  (64, 78)  ...   \n",
      "2  xhcEpYg3dKnzts5ejIiSBJ  1658912403000       53       68  (53, 68)  ...   \n",
      "3  j6nwjs91i2ihz003f4gtbv  1592841017038       46       49  (46, 49)  ...   \n",
      "4  NZkG7hnM5loCSLwiAHjIVw  1460887821000       76       46  (76, 46)  ...   \n",
      "\n",
      "      region  sub-region                     city  \\\n",
      "0      Boeny         NaN                Sitampiky   \n",
      "1    Aqtoebe         NaN                   Martuk   \n",
      "2     Sicily     Messina  Santa Domenica Vittoria   \n",
      "3  Koulikoro         NaN                  Kangaba   \n",
      "4      Krabi         NaN                 Ko Lanta   \n",
      "\n",
      "                                 unique_city  unique_sub-region  \\\n",
      "0                     Sitampiky_NaN_Boeny_MG                NaN   \n",
      "1                      Martuk_NaN_Aqtoebe_KZ                NaN   \n",
      "2  Santa Domenica Vittoria_Messina_Sicily_IT  Messina_Sicily_IT   \n",
      "3                   Kangaba_NaN_Koulikoro_ML                NaN   \n",
      "4                      Ko Lanta_NaN_Krabi_TH                NaN   \n",
      "\n",
      "   unique_region unique_country quadtree_10_1000 creator_username  \\\n",
      "0       Boeny_MG             MG             1509           eric_s   \n",
      "1     Aqtoebe_KZ             KZ            10995          vik1607   \n",
      "2      Sicily_IT             IT             6488        gpsmapper   \n",
      "3   Koulikoro_ML             ML             3362       diallo1960   \n",
      "4       Krabi_TH             TH             1804           soesoe   \n",
      "\n",
      "     creator_id  \n",
      "0  1.078376e+14  \n",
      "1  1.110825e+14  \n",
      "2  1.006127e+14  \n",
      "3  1.009980e+14  \n",
      "4  1.071101e+14  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download the test CSV\n",
    "csv_path = hf_hub_download(\n",
    "    repo_id=\"osv5m/osv5m\",\n",
    "    repo_type=\"dataset\",\n",
    "    filename=\"test.csv\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Downloaded test.csv to: {csv_path}\")\n",
    "\n",
    "# Load and inspect it\n",
    "import pandas as pd\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"\\nüìä Test set size: {len(df)} images\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90772a4",
   "metadata": {},
   "source": [
    "## Randooooooom sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1216cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sampled 50000 images\n",
      "\n",
      "Country distribution (top 10):\n",
      "country\n",
      "US    5596\n",
      "RU    3498\n",
      "AU    2572\n",
      "BR    2417\n",
      "CA    2274\n",
      "IN    1560\n",
      "MX    1138\n",
      "CN    1087\n",
      "AR    1032\n",
      "KZ     922\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Region distribution (top 10):\n",
      "region\n",
      "New South Wales      632\n",
      "Queensland           604\n",
      "Ontario              551\n",
      "Texas                524\n",
      "Western Australia    514\n",
      "British Columbia     473\n",
      "Montana              370\n",
      "South Australia      348\n",
      "Minas Gerais         332\n",
      "Bahia                326\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing labels:\n",
      "  Country: 0\n",
      "  Region: 434\n",
      "  City: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Sample 5000 images randomly\n",
    "np.random.seed(42)\n",
    "df_sample = df.sample(n=50000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Sampled {len(df_sample)} images\")\n",
    "print(f\"\\nCountry distribution (top 10):\")\n",
    "print(df_sample['country'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\nRegion distribution (top 10):\")\n",
    "print(df_sample['region'].value_counts().head(10))\n",
    "\n",
    "# Check for missing labels\n",
    "print(f\"\\nMissing labels:\")\n",
    "print(f\"  Country: {df_sample['country'].isna().sum()}\")\n",
    "print(f\"  Region: {df_sample['region'].isna().sum()}\")\n",
    "print(f\"  City: {df_sample['city'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efc80f",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0461183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Imed\\anaconda3\\envs\\plonk\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 00.zip (2.10 GB)...\n",
      "URL: https://huggingface.co/datasets/osv5m/osv5m/resolve/main/images/test/00.zip\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.25G/2.25G [03:45<00:00, 9.99MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Downloaded to datasets/osv5m/00.zip\n",
      "Size: 2.10 GB\n",
      "\n",
      "Extracting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50001/50001 [00:50<00:00, 997.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! You have ~42,000 images now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Get the download URL for just the first zip\n",
    "from huggingface_hub import hf_hub_url\n",
    "\n",
    "url = hf_hub_url(\n",
    "    repo_id=\"osv5m/osv5m\",\n",
    "    filename=\"images/test/00.zip\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Downloading 00.zip (2.10 GB)...\")\n",
    "print(f\"URL: {url}\\n\")\n",
    "\n",
    "os.makedirs(\"datasets/osv5m/images/test\", exist_ok=True)\n",
    "\n",
    "# Download with visible progress bar\n",
    "response = requests.get(url, stream=True, timeout=30)\n",
    "total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "output_path = \"datasets/osv5m/00.zip\"\n",
    "\n",
    "with open(output_path, 'wb') as file:\n",
    "    with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "print(f\"\\n‚úÖ Downloaded to {output_path}\")\n",
    "print(f\"Size: {os.path.getsize(output_path) / (1024**3):.2f} GB\")\n",
    "\n",
    "# Extract it\n",
    "import zipfile\n",
    "print(\"\\nExtracting...\")\n",
    "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "    members = zip_ref.namelist()\n",
    "    for member in tqdm(members, desc=\"Extracting\"):\n",
    "        zip_ref.extract(member, \"datasets/osv5m/images/test\")\n",
    "\n",
    "print(\"‚úÖ Done! You have ~42,000 images now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c832cdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Downloaded images: 50,000\n",
      "Example downloaded ID: 167304391973915\n",
      "Type: <class 'str'>\n",
      "\n",
      "Example CSV ID: 547473234108938\n",
      "Type: <class 'numpy.int64'>\n",
      "\n",
      "‚úÖ Matched: 50,000 images\n",
      "\n",
      "Country distribution (top 10):\n",
      "country\n",
      "US    5689\n",
      "RU    3622\n",
      "AU    2630\n",
      "BR    2427\n",
      "CA    2303\n",
      "IN    1478\n",
      "MX    1147\n",
      "AR    1026\n",
      "CN    1022\n",
      "KZ     923\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Final dataset: 50,000 images\n",
      "Countries: 217\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "image_dir = r\"C:\\Users\\Imed\\Desktop\\VMI\\Project\\Dev\\plonk\\datasets\\osv5m\\images\\test\\00\"\n",
    "csv_path = r\"C:\\Users\\Imed\\.cache\\huggingface\\hub\\datasets--osv5m--osv5m\\snapshots\\cff33609b56b54d8743b7ee7a416eb8433e9a681\\test.csv\"\n",
    "\n",
    "# Load CSV\n",
    "df_full = pd.read_csv(csv_path)\n",
    "\n",
    "# Get image IDs from files\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "downloaded_ids = set([f.replace('.jpg', '') for f in image_files])\n",
    "\n",
    "print(f\"üì∏ Downloaded images: {len(downloaded_ids):,}\")\n",
    "print(f\"Example downloaded ID: {list(downloaded_ids)[0]}\")\n",
    "print(f\"Type: {type(list(downloaded_ids)[0])}\\n\")\n",
    "\n",
    "print(f\"Example CSV ID: {df_full['id'].iloc[0]}\")\n",
    "print(f\"Type: {type(df_full['id'].iloc[0])}\\n\")\n",
    "\n",
    "# Convert both to strings for matching\n",
    "df_full['id_str'] = df_full['id'].astype(str)\n",
    "df_matched = df_full[df_full['id_str'].isin(downloaded_ids)].copy()\n",
    "\n",
    "print(f\"‚úÖ Matched: {len(df_matched):,} images\")\n",
    "print(f\"\\nCountry distribution (top 10):\")\n",
    "print(df_matched['country'].value_counts().head(10))\n",
    "\n",
    "# Sample 50000\n",
    "df_sample = df_matched.sample(n=min(50000, len(df_matched)), random_state=42).reset_index(drop=True)\n",
    "print(f\"\\n‚úÖ Final dataset: {len(df_sample):,} images\")\n",
    "print(f\"Countries: {df_sample['country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb380e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we have df_sample saved?\n",
      "df_sample exists: True\n",
      "\n",
      "‚úÖ We have 50000 images with labels\n",
      "Columns: ['id', 'latitude', 'longitude', 'thumb_original_url', 'country', 'sequence', 'captured_at', 'lon_bin', 'lat_bin', 'cell']\n",
      "\n",
      "First row:\n",
      "id           301474474887470\n",
      "country                   RU\n",
      "region                 Sakha\n",
      "latitude           62.628466\n",
      "longitude         135.889643\n",
      "Name: 0, dtype: object\n",
      "\n",
      "============================================================\n",
      "PLONK Model Check:\n",
      "============================================================\n",
      "\n",
      "Model path: nicolas-dufour/PLONK_OSV_5M\n",
      "‚úÖ Has cond_preprocessing (image encoder)\n",
      "\n",
      "Pipeline structure:\n",
      "  ‚úÖ network: <class 'plonk.models.pretrained_models.Plonk'>\n",
      "  ‚úÖ preconditioning: <class 'plonk.models.preconditioning.DDPMPrecond'>\n"
     ]
    }
   ],
   "source": [
    "# 1. Check the matched dataset\n",
    "print(\"Do we have df_sample saved?\")\n",
    "print(f\"df_sample exists: {'df_sample' in locals()}\")\n",
    "\n",
    "if 'df_sample' in locals():\n",
    "    print(f\"\\n‚úÖ We have {len(df_sample)} images with labels\")\n",
    "    print(f\"Columns: {df_sample.columns.tolist()[:10]}\")\n",
    "    print(f\"\\nFirst row:\")\n",
    "    print(df_sample.iloc[0][['id', 'country', 'region', 'latitude', 'longitude']])\n",
    "else:\n",
    "    print(\"‚ùå Need to rerun matching\")\n",
    "\n",
    "# 2. Check what model we loaded\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PLONK Model Check:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nModel path: {pipeline.model_path if hasattr(pipeline, 'model_path') else 'Unknown'}\")\n",
    "\n",
    "# Check the image encoder\n",
    "if hasattr(pipeline, 'cond_preprocessing'):\n",
    "    print(\"‚úÖ Has cond_preprocessing (image encoder)\")\n",
    "    \n",
    "# What's the actual backbone?\n",
    "print(\"\\nPipeline structure:\")\n",
    "for attr in ['network', 'preconditioning', 'image_encoder', 'backbone']:\n",
    "    if hasattr(pipeline, attr):\n",
    "        obj = getattr(pipeline, attr)\n",
    "        print(f\"  ‚úÖ {attr}: {type(obj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d453b",
   "metadata": {},
   "source": [
    "## Just some code to see the feature extractor inside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b12ec9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extractor: <class 'plonk.pipe.StreetClipFeatureExtractor'>\n",
      "Type: StreetClipFeatureExtractor\n",
      "\n",
      "Feature extractor attributes:\n",
      "  - device\n",
      "  - emb_model\n",
      "  - processor\n",
      "\n",
      "============================================================\n",
      "Testing feature extraction:\n",
      "============================================================\n",
      "Output type: <class 'dict'>\n",
      "Keys: dict_keys(['img', 'emb'])\n",
      "  'img': <class 'list'>\n",
      "  'emb': shape torch.Size([1, 1024]), dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Get the feature extractor\n",
    "feature_extractor = pipeline.cond_preprocessing\n",
    "\n",
    "print(f\"Feature Extractor: {type(feature_extractor)}\")\n",
    "print(f\"Type: {feature_extractor.__class__.__name__}\\n\")\n",
    "\n",
    "# Check its attributes\n",
    "print(\"Feature extractor attributes:\")\n",
    "for attr in dir(feature_extractor):\n",
    "    if not attr.startswith('_'):\n",
    "        print(f\"  - {attr}\")\n",
    "\n",
    "# Try to get the actual model\n",
    "if hasattr(feature_extractor, 'model'):\n",
    "    print(f\"\\n‚úÖ Has model: {type(feature_extractor.model)}\")\n",
    "if hasattr(feature_extractor, 'backbone'):\n",
    "    print(f\"‚úÖ Has backbone: {type(feature_extractor.backbone)}\")\n",
    "if hasattr(feature_extractor, 'encoder'):\n",
    "    print(f\"‚úÖ Has encoder: {type(feature_extractor.encoder)}\")\n",
    "\n",
    "# Test extraction\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing feature extraction:\")\n",
    "print(\"=\"*60)\n",
    "# The feature extractor wants: {\"img\": [list of images]}\n",
    "\n",
    "test_img = Image.open(os.path.join(image_dir, f\"{df_sample.iloc[0]['id']}.jpg\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Correct format: dict with 'img' key\n",
    "    result = feature_extractor({\"img\": [test_img]})\n",
    "\n",
    "print(f\"Output type: {type(result)}\")\n",
    "if isinstance(result, dict):\n",
    "    print(f\"Keys: {result.keys()}\")\n",
    "    for key, value in result.items():\n",
    "        if torch.is_tensor(value):\n",
    "            print(f\"  '{key}': shape {value.shape}, dtype {value.dtype}\")\n",
    "        else:\n",
    "            print(f\"  '{key}': {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c186e66",
   "metadata": {},
   "source": [
    "# Extracting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f77c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting StreetCLIP features from 5,000 images...\n",
      "Feature dimension: 1024\n",
      "Estimated time: ~10-15 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 782/782 [1:21:10<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Extraction complete!\n",
      "Features: (50000, 1024)\n",
      "Countries: 217 unique\n",
      "Regions: 2050 unique\n",
      "\n",
      "üíæ Saved features to disk\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"Extracting StreetCLIP features from 50,000 images...\")\n",
    "print(\"Feature dimension: 1024\")\n",
    "\n",
    "features_list = []\n",
    "countries_list = []\n",
    "regions_list = []\n",
    "indices_list = []\n",
    "\n",
    "batch_size = 64  # Larger batch for faster extraction\n",
    "\n",
    "for i in tqdm(range(0, len(df_sample), batch_size), desc=\"Extracting\"):\n",
    "    batch_df = df_sample.iloc[i:i+batch_size]\n",
    "    \n",
    "    # Load images for this batch\n",
    "    images = []\n",
    "    valid_rows = []\n",
    "    \n",
    "    for idx, row in batch_df.iterrows():\n",
    "        img_path = os.path.join(image_dir, f\"{row['id']}.jpg\")\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            images.append(img)\n",
    "            valid_rows.append(row)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        batch_dict = feature_extractor({\"img\": images})\n",
    "        features = batch_dict['emb'].cpu().numpy()\n",
    "    \n",
    "    # Store features and labels\n",
    "    for feat_idx, row in enumerate(valid_rows):\n",
    "        features_list.append(features[feat_idx])\n",
    "        countries_list.append(row['country'])\n",
    "        regions_list.append(row['region'] if pd.notna(row['region']) else 'UNKNOWN')\n",
    "        indices_list.append(row.name)\n",
    "\n",
    "# Convert to numpy\n",
    "X = np.array(features_list)\n",
    "y_country = np.array(countries_list)\n",
    "y_region = np.array(regions_list)\n",
    "\n",
    "print(f\"\\n‚úÖ Extraction complete!\")\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Countries: {len(np.unique(y_country))} unique\")\n",
    "print(f\"Regions: {len(np.unique(y_region))} unique\")\n",
    "\n",
    "# Save for later use\n",
    "np.save('streetclip_features.npy', X)\n",
    "np.save('country_labels.npy', y_country)\n",
    "np.save('region_labels.npy', y_region)\n",
    "print(f\"\\nüíæ Saved features to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa392cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RELOADING FEATURES & RETRAINING LINEAR PROBE\n",
      "============================================================\n",
      "\n",
      "1. Loading features from disk...\n",
      "‚úÖ Features loaded: (50000, 1024)\n",
      "‚úÖ Labels loaded: (50000,)\n",
      "   Unique countries: 217\n",
      "\n",
      "2. Filtering countries...\n",
      "   Total countries: 217\n",
      "   Countries with ‚â•2 samples: 210\n",
      "   Samples kept: 49993/50000 (100.0%)\n",
      "\n",
      "3. Splitting train/test...\n",
      "   Train: 39994 samples\n",
      "   Test: 9999 samples\n",
      "\n",
      "4. Training logistic regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Evaluating...\n",
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n",
      "Top-1 Accuracy: 84.97%\n",
      "Top-5 Accuracy: 97.26%\n",
      "Random Baseline: 0.48%\n",
      "Improvement: 178.4x\n",
      "\n",
      "============================================================\n",
      "SAVING TO DISK\n",
      "============================================================\n",
      "‚úÖ Saved: country_classifier.pkl\n",
      "‚úÖ Saved: train/test splits & predictions\n",
      "‚úÖ Saved: results.npy\n",
      "\n",
      "============================================================\n",
      "‚úÖ ALL DONE! Everything saved to disk.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RELOADING FEATURES & RETRAINING LINEAR PROBE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Load saved features\n",
    "print(\"\\n1. Loading features from disk...\")\n",
    "X = np.load('streetclip_features.npy')\n",
    "y_country = np.load('country_labels.npy')\n",
    "\n",
    "print(f\"‚úÖ Features loaded: {X.shape}\")\n",
    "print(f\"‚úÖ Labels loaded: {y_country.shape}\")\n",
    "print(f\"   Unique countries: {len(np.unique(y_country))}\")\n",
    "\n",
    "# 2. Filter countries with >=2 samples\n",
    "print(\"\\n2. Filtering countries...\")\n",
    "unique, counts = np.unique(y_country, return_counts=True)\n",
    "valid_countries = unique[counts >= 2]\n",
    "\n",
    "print(f\"   Total countries: {len(unique)}\")\n",
    "print(f\"   Countries with ‚â•2 samples: {len(valid_countries)}\")\n",
    "\n",
    "valid_mask = np.isin(y_country, valid_countries)\n",
    "X_filtered = X[valid_mask]\n",
    "y_filtered = y_country[valid_mask]\n",
    "\n",
    "print(f\"   Samples kept: {len(X_filtered)}/{len(X)} ({100*len(X_filtered)/len(X):.1f}%)\")\n",
    "\n",
    "# 3. Split train/test\n",
    "print(\"\\n3. Splitting train/test...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n",
    ")\n",
    "\n",
    "print(f\"   Train: {len(X_train)} samples\")\n",
    "print(f\"   Test: {len(X_test)} samples\")\n",
    "\n",
    "# 4. Train classifier\n",
    "print(\"\\n4. Training logistic regression...\")\n",
    "country_clf = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1, verbose=1)\n",
    "country_clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate\n",
    "print(\"\\n5. Evaluating...\")\n",
    "y_pred = country_clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Top-5 accuracy\n",
    "probs = country_clf.predict_proba(X_test)\n",
    "top5_preds = np.argsort(probs, axis=1)[:, -5:]\n",
    "top5_acc = np.mean([y_test[i] in country_clf.classes_[top5_preds[i]] \n",
    "                     for i in range(len(y_test))])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Top-1 Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {top5_acc*100:.2f}%\")\n",
    "print(f\"Random Baseline: {100.0/len(valid_countries):.2f}%\")\n",
    "print(f\"Improvement: {acc/(1.0/len(valid_countries)):.1f}x\")\n",
    "\n",
    "# 6. Save everything\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAVING TO DISK\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save the trained model\n",
    "with open('country_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(country_clf, f)\n",
    "print(\"‚úÖ Saved: country_classifier.pkl\")\n",
    "\n",
    "# Save train/test splits\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)\n",
    "np.save('y_pred.npy', y_pred)\n",
    "print(\"‚úÖ Saved: train/test splits & predictions\")\n",
    "\n",
    "# Save results summary\n",
    "results = {\n",
    "    'accuracy': acc,\n",
    "    'top5_accuracy': top5_acc,\n",
    "    'n_countries': len(valid_countries),\n",
    "    'n_train': len(X_train),\n",
    "    'n_test': len(X_test),\n",
    "}\n",
    "\n",
    "np.save('results.npy', results)\n",
    "print(\"‚úÖ Saved: results.npy\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ ALL DONE! Everything saved to disk.\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a78d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LINEAR PROBING EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter out countries with < 2 samples (needed for stratified split)\n",
    "unique, counts = np.unique(y_country, return_counts=True)\n",
    "valid_countries = unique[counts >= 2]\n",
    "\n",
    "print(f\"\\nFiltering countries:\")\n",
    "print(f\"  Total countries: {len(unique)}\")\n",
    "print(f\"  Countries with ‚â•2 samples: {len(valid_countries)}\")\n",
    "\n",
    "# Keep only samples from valid countries\n",
    "valid_mask = np.isin(y_country, valid_countries)\n",
    "X_filtered = X[valid_mask]\n",
    "y_filtered = y_country[valid_mask]\n",
    "\n",
    "print(f\"  Samples kept: {len(X_filtered)}/{len(X)} ({100*len(X_filtered)/len(X):.1f}%)\")\n",
    "\n",
    "# Split train/test (80/20) with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Train: {len(X_train)} images\")\n",
    "print(f\"  Test:  {len(X_test)} images\")\n",
    "\n",
    "# ============================================================\n",
    "# COUNTRY CLASSIFICATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COUNTRY CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Training classifier for {len(valid_countries)} countries...\")\n",
    "country_clf = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1, verbose=1)\n",
    "country_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = country_clf.predict(X_test)\n",
    "country_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Top-5 accuracy\n",
    "country_probs = country_clf.predict_proba(X_test)\n",
    "top5_preds = np.argsort(country_probs, axis=1)[:, -5:]\n",
    "top5_acc = np.mean([y_test[i] in country_clf.classes_[top5_preds[i]] \n",
    "                     for i in range(len(y_test))])\n",
    "\n",
    "# Random baseline\n",
    "random_baseline = 1.0 / len(valid_countries)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nCountry Classification ({len(valid_countries)} classes):\")\n",
    "print(f\"  ‚úÖ Top-1 Accuracy: {country_acc*100:.2f}%\")\n",
    "print(f\"  ‚úÖ Top-5 Accuracy: {top5_acc*100:.2f}%\")\n",
    "print(f\"  üìä Random Baseline: {random_baseline*100:.2f}%\")\n",
    "print(f\"  üöÄ Improvement: {country_acc/random_baseline:.1f}x better than random\")\n",
    "\n",
    "# Most confused countries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=country_clf.classes_)\n",
    "print(f\"\\nPer-country accuracy (top 10 by sample count):\")\n",
    "for country in valid_countries[:10]:\n",
    "    if country in y_test:\n",
    "        country_mask = y_test == country\n",
    "        if country_mask.sum() > 0:\n",
    "            country_acc_individual = (y_pred[country_mask] == country).mean()\n",
    "            print(f\"  {country}: {country_acc_individual*100:.1f}% ({country_mask.sum()} test samples)\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'country_accuracy': country_acc,\n",
    "    'top5_accuracy': top5_acc,\n",
    "    'n_countries': len(valid_countries),\n",
    "    'n_train': len(X_train),\n",
    "    'n_test': len(X_test),\n",
    "    'random_baseline': random_baseline\n",
    "}\n",
    "\n",
    "np.save('probe_results.npy', results) # type: ignore\n",
    "print(f\"\\nüíæ Results saved to probe_results.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70770eff",
   "metadata": {},
   "source": [
    "# Testing on random images from the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1828f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing image: C:\\Users\\Imed\\Desktop\\VMI\\Project\\Dev\\plonk\\datasets\\other\\20170411_manchester-salford-quays-bridge_16-9.avif\n",
      "Expected country: AU\n",
      "============================================================\n",
      "‚úÖ Image loaded: (1920, 1080)\n",
      "\n",
      "üéØ Top-1 Prediction: CN\n",
      "‚ùå Wrong (expected AU)\n",
      "\n",
      "Top 5 predictions:\n",
      "   1. CN: 94.61%\n",
      "   2. RU: 1.83%\n",
      "   3. EG: 1.29%\n",
      "   4. GB: 1.28%\n",
      "   5. NG: 0.41%\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def test_local_image(image_path, country_name=\"Unknown\"):\n",
    "    \"\"\"Test PLONK on a local image\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing image: {image_path}\")\n",
    "    print(f\"Expected country: {country_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load image\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        print(f\"‚úÖ Image loaded: {img.size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load image: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        batch_dict = feature_extractor({\"img\": [img]})\n",
    "        features = batch_dict['emb'].cpu().numpy()\n",
    "    \n",
    "    # Predict\n",
    "    prediction = country_clf.predict(features)[0]\n",
    "    probabilities = country_clf.predict_proba(features)[0]\n",
    "    \n",
    "    # Top 5\n",
    "    top5_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "    top5_countries = country_clf.classes_[top5_indices]\n",
    "    top5_probs = probabilities[top5_indices]\n",
    "    \n",
    "    print(f\"\\nüéØ Top-1 Prediction: {prediction}\")\n",
    "    if prediction == country_name:\n",
    "        print(f\"‚úÖ CORRECT!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Wrong (expected {country_name})\")\n",
    "    \n",
    "    print(f\"\\nTop 5 predictions:\")\n",
    "    for i, (country, prob) in enumerate(zip(top5_countries, top5_probs)):\n",
    "        marker = \"‚úÖ\" if country == country_name else \"  \"\n",
    "        print(f\"{marker} {i+1}. {country}: {prob*100:.2f}%\")\n",
    "    \n",
    "    # Display\n",
    "    img.show()\n",
    "    return img\n",
    "\n",
    "# Test your local image\n",
    "img = test_local_image(\"C:\\\\Users\\\\Imed\\\\Desktop\\\\VMI\\\\Project\\\\Dev\\\\plonk\\\\datasets\\\\other\\\\20170411_manchester-salford-quays-bridge_16-9.avif\", country_name=\"AU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca29db5",
   "metadata": {},
   "source": [
    "# City level probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b557b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CITY-LEVEL CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Load features (already extracted!)\n",
    "X = np.load('streetclip_features.npy')\n",
    "y_country = np.load('country_labels.npy')\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded features: {X.shape}\")\n",
    "\n",
    "# 2. Get city labels from df_sample\n",
    "print(\"\\nüìç Extracting city labels...\")\n",
    "\n",
    "# Create city labels (need unique identifier: city + country)\n",
    "city_labels = []\n",
    "for idx in range(len(df_sample)):\n",
    "    city = df_sample.iloc[idx]['city']\n",
    "    country = df_sample.iloc[idx]['country']\n",
    "    \n",
    "    # Handle missing cities\n",
    "    if pd.isna(city) or city == '':\n",
    "        city_label = f\"UNKNOWN_{country}\"\n",
    "    else:\n",
    "        # Make unique: \"Paris_FR\", \"Paris_US\" are different\n",
    "        city_label = f\"{city}_{country}\"\n",
    "    \n",
    "    city_labels.append(city_label)\n",
    "\n",
    "y_city = np.array(city_labels)\n",
    "\n",
    "print(f\"Total cities: {len(np.unique(y_city))}\")\n",
    "\n",
    "# 3. Filter cities with >= 2 samples (needed for stratified split)\n",
    "unique_cities, counts = np.unique(y_city, return_counts=True)\n",
    "valid_cities = unique_cities[counts >= 2]\n",
    "\n",
    "print(f\"Cities with ‚â•2 samples: {len(valid_cities)}\")\n",
    "\n",
    "valid_mask = np.isin(y_city, valid_cities)\n",
    "X_filtered = X[valid_mask]\n",
    "y_filtered = y_city[valid_mask]\n",
    "\n",
    "print(f\"Samples kept: {len(X_filtered)}/{len(X)} ({100*len(X_filtered)/len(X):.1f}%)\")\n",
    "\n",
    "# 4. Split train/test\n",
    "print(\"\\nüîÄ Splitting train/test...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "\n",
    "# 5. Train classifier (this should be fast - features already extracted!)\n",
    "print(f\"\\nüöÄ Training logistic regression for {len(valid_cities)} cities...\")\n",
    "\n",
    "city_clf = LogisticRegression(\n",
    "    solver='saga',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    C=1.0,\n",
    "    multi_class='multinomial'\n",
    ")\n",
    "\n",
    "city_clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate\n",
    "print(\"\\nüìä Evaluating...\")\n",
    "y_pred = city_clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Top-5 accuracy\n",
    "probs = city_clf.predict_proba(X_test)\n",
    "top5_preds = np.argsort(probs, axis=1)[:, -5:]\n",
    "top5_acc = np.mean([y_test[i] in city_clf.classes_[top5_preds[i]] \n",
    "                     for i in range(len(y_test))])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CITY CLASSIFICATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Top-1 Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {top5_acc*100:.2f}%\")\n",
    "print(f\"Random Baseline: {100.0/len(valid_cities):.3f}%\")\n",
    "print(f\"Improvement: {acc/(1.0/len(valid_cities)):.0f}x\")\n",
    "\n",
    "# 7. Save everything\n",
    "print(f\"\\nüíæ Saving...\")\n",
    "with open('city_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(city_clf, f)\n",
    "\n",
    "np.save('y_city_test.npy', y_test)\n",
    "np.save('y_city_pred.npy', y_pred)\n",
    "\n",
    "print(\"‚úÖ Saved: city_classifier.pkl\")\n",
    "print(\"‚úÖ Saved: city test labels & predictions\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ CITY CLASSIFIER TRAINED!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656919d",
   "metadata": {},
   "source": [
    "# Region linear probing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07032cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REGION LINEAR PROBE\n",
    "# ============================================================\n",
    "print(\"=\"*60)\n",
    "print(\"REGION CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Load features and region labels\n",
    "X = np.load('streetclip_features.npy')\n",
    "y_region = np.load('region_labels.npy')\n",
    "\n",
    "print(f\"‚úÖ Features loaded: {X.shape}\")\n",
    "print(f\"‚úÖ Region labels loaded: {y_region.shape}\")\n",
    "\n",
    "# 2. Filter regions with ‚â•2 samples\n",
    "unique_regions, counts = np.unique(y_region, return_counts=True)\n",
    "valid_regions = unique_regions[counts >= 2]\n",
    "valid_mask = np.isin(y_region, valid_regions)\n",
    "\n",
    "print(f\"   Total regions: {len(unique_regions)}\")\n",
    "print(f\"   Regions with ‚â•2 samples: {len(valid_regions)}\")\n",
    "print(f\"   Samples kept: {valid_mask.sum()}/{len(y_region)} ({valid_mask.sum()/len(y_region)*100:.1f}%)\")\n",
    "\n",
    "# 3. Filter features and labels\n",
    "X_filtered = X[valid_mask]\n",
    "y_filtered = y_region[valid_mask]\n",
    "\n",
    "# 4. Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y_filtered, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_filtered\n",
    ")\n",
    "\n",
    "print(f\"   Train: {len(X_train)} samples\")\n",
    "print(f\"   Test: {len(X_test)} samples\")\n",
    "\n",
    "# 5. Train classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"4. Training logistic regression...\")\n",
    "clf_region = LogisticRegression(\n",
    "    max_iter=1000, \n",
    "    n_jobs=-1,  # Use all cores\n",
    "    random_state=42,\n",
    "    verbose=1   # Show progress\n",
    ")\n",
    "\n",
    "clf_region.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate\n",
    "print(\"5. Evaluating...\")\n",
    "\n",
    "# Top-1 accuracy\n",
    "y_pred = clf_region.predict(X_test)\n",
    "top1_acc = np.mean(y_pred == y_test)\n",
    "\n",
    "# Top-5 accuracy\n",
    "y_proba = clf_region.predict_proba(X_test)\n",
    "top5_pred = np.argsort(y_proba, axis=1)[:, -5:]\n",
    "top5_acc = np.mean([y_test[i] in top5_pred[i] for i in range(len(y_test))])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REGION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Top-1 Accuracy: {top1_acc*100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {top5_acc*100:.2f}%\")\n",
    "print(f\"Classes: {len(valid_regions)}\")\n",
    "\n",
    "# 7. Save\n",
    "import pickle\n",
    "with open('region_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_region, f)\n",
    "    \n",
    "np.save('X_train_region.npy', X_train)\n",
    "np.save('X_test_region.npy', X_test)\n",
    "np.save('y_train_region.npy', y_train)\n",
    "np.save('y_test_region.npy', y_test)\n",
    "np.save('y_pred_region.npy', y_pred)\n",
    "\n",
    "print(\"‚úÖ Saved: region_classifier.pkl\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446aeb7",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be1b4b7",
   "metadata": {},
   "source": [
    "# our test set on plonk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55844e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLONK FULL MODEL COMPARISON (100 images)\n",
    "# ============================================================\n",
    "from plonk import PlonkPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING PLONK FULL MODEL ON 100 IMAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load PLONK - CORRECT WAY\n",
    "pipeline = PlonkPipeline(\"nicolas-dufour/PLONK_OSV_5M\")\n",
    "\n",
    "# Prepare centroids for reverse geocoding\n",
    "country_centroids = df_sample.groupby('country')[['latitude', 'longitude']].mean()\n",
    "region_centroids = df_sample.groupby('region')[['latitude', 'longitude']].mean()\n",
    "city_centroids = df_sample.groupby('city')[['latitude', 'longitude']].mean()\n",
    "\n",
    "def gps_to_label(lat, lon, centroids):\n",
    "    \"\"\"Find nearest centroid\"\"\"\n",
    "    min_dist = float('inf')\n",
    "    best_label = None\n",
    "    for label in centroids.index:\n",
    "        c_lat = centroids.loc[label, 'latitude']\n",
    "        c_lon = centroids.loc[label, 'longitude']\n",
    "        dist = ((lat - c_lat)**2 + (lon - c_lon)**2)**0.5\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_label = label\n",
    "    return best_label\n",
    "\n",
    "# Get test indices (first 10000 sampled)\n",
    "test_sample = test_idx[:10000]\n",
    "\n",
    "plonk_country_correct = 0\n",
    "plonk_region_correct = 0\n",
    "plonk_city_correct = 0\n",
    "\n",
    "image_dir = r\"C:\\Users\\Imed\\Desktop\\VMI\\Project\\Dev\\plonk\\datasets\\osv5m\\images\\test\\00\"\n",
    "\n",
    "for i in tqdm(range(10000), desc=\"PLONK inference\"):\n",
    "    orig_idx = test_sample[i]\n",
    "    \n",
    "    # Get image\n",
    "    img_id = df_sample.iloc[orig_idx]['id']\n",
    "    img_path = os.path.join(image_dir, f\"{img_id}.jpg\")\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # PLONK prediction - returns GPS coordinates\n",
    "        gps_coords = pipeline([img], batch_size=1)  # List of images\n",
    "        pred_lat, pred_lon = float(gps_coords[0][0]), float(gps_coords[0][1])\n",
    "        \n",
    "        # True labels\n",
    "        true_country = df_sample.iloc[orig_idx]['country']\n",
    "        true_region = df_sample.iloc[orig_idx]['region']\n",
    "        true_city = df_sample.iloc[orig_idx]['city']\n",
    "        \n",
    "        # Reverse geocode\n",
    "        pred_country = gps_to_label(pred_lat, pred_lon, country_centroids)\n",
    "        pred_region = gps_to_label(pred_lat, pred_lon, region_centroids)\n",
    "        pred_city = gps_to_label(pred_lat, pred_lon, city_centroids)\n",
    "        \n",
    "        # Count correct\n",
    "        if pred_country == true_country:\n",
    "            plonk_country_correct += 1\n",
    "        if pred_region == true_region:\n",
    "            plonk_region_correct += 1\n",
    "        if pred_city == true_city:\n",
    "            plonk_city_correct += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error on {img_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PLONK RESULTS (10000 images)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Country: {plonk_country_correct}%\")\n",
    "print(f\"Region:  {plonk_region_correct}%\")\n",
    "print(f\"City:    {plonk_city_correct}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plonk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
